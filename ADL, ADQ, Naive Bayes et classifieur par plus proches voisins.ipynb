{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation de méthodes élémentaires pour la classification supervisée : ADL, ADQ, Naive Bayes et classifieur par plus proches voisins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous aurons besoin des modules Python ci-dessous, il vous faut donc évidemment exécuter cette première cellule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from scipy.io import arff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données [Vertebral Column](https://archive.ics.uci.edu/ml/datasets/Vertebral+Column) permet d'étudier les pathologies d'hernie discale et de Spondylolisthesis. Ces deux pathologies sont regroupées dans le jeu de données en une seule catégorie dite `Abnormale`. \n",
    "\n",
    "Il s'agit donc d'un problème de classification supervisée à deux classes :\n",
    "- Normale (NO) \n",
    "- Abnormale (AB)    \n",
    "\n",
    "avec 6 variables bio-mécaniques disponibles (features).\n",
    "\n",
    "L'objectif du TP est d'implémenter quelques méthodes simples de classification supervisée pour ce problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Télécharger le fichier column_2C.dat depuis le site de l'UCI à [cette adresse](https://archive.ics.uci.edu/ml/datasets/Vertebral+Column). \n",
    ">\n",
    "> On peut importer les données sous python par exemple avec la librairie [pandas](https://pandas.pydata.org/pandas-docs/stable/10min.html). Vous pourrez au besoin consulter la documentation de la fonction [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html). \n",
    "> \n",
    "> Le chemin donné dans la fonction `read_csv`est une chaîne de caractère qui spécifie le chemin complet vers le ficher sur votre machine. On peut aussi donner une adresse url si le fichier est disponible en ligne.\n",
    ">\n",
    "> Attention à la syntaxe pour les chemins sous Windows doit etre de la forme  `C:/truc/machin.csv`. \n",
    "> \n",
    "> Voir ce [blog](https://medium.com/@ageitgey/python-3-quick-tip-the-easy-way-to-deal-with-file-paths-on-windows-mac-and-linux-11a072b58d5f) pour en savoir plus sur la \"manipulation des chemins\" sur des OS variés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path= r\"C:\\Users\\kassi\\Downloads\\vertebral+column\\column_2C_weka.arff\"\n",
    "data, meta = arff.loadarff(file_path)\n",
    "Vertebral = pd.DataFrame(data)\n",
    "Vertebral.columns = [\"pelvic_incidence\",\n",
    "                                       \"pelvic_tilt\",\n",
    "                                       \"lumbar_lordosis_angle\",\n",
    "                                       \"sacral_slope\",\n",
    "                                       \"pelvic_radius\",\n",
    "                                       \"degree_spondylolisthesis\",\n",
    "                                       \"class\"]\n",
    "Vertebral[\"class\"] = Vertebral[\"class\"].apply( lambda x: \"Abnormal\" if x==b'Abnormal' else 'Normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vérifier à l'aide des méthodes `.head()`  et `describe()` que les données sont bien importées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pelvic_incidence</th>\n",
       "      <th>pelvic_tilt</th>\n",
       "      <th>lumbar_lordosis_angle</th>\n",
       "      <th>sacral_slope</th>\n",
       "      <th>pelvic_radius</th>\n",
       "      <th>degree_spondylolisthesis</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.027817</td>\n",
       "      <td>22.552586</td>\n",
       "      <td>39.609117</td>\n",
       "      <td>40.475232</td>\n",
       "      <td>98.672917</td>\n",
       "      <td>-0.254400</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.056951</td>\n",
       "      <td>10.060991</td>\n",
       "      <td>25.015378</td>\n",
       "      <td>28.995960</td>\n",
       "      <td>114.405425</td>\n",
       "      <td>4.564259</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.832021</td>\n",
       "      <td>22.218482</td>\n",
       "      <td>50.092194</td>\n",
       "      <td>46.613539</td>\n",
       "      <td>105.985135</td>\n",
       "      <td>-3.530317</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.297008</td>\n",
       "      <td>24.652878</td>\n",
       "      <td>44.311238</td>\n",
       "      <td>44.644130</td>\n",
       "      <td>101.868495</td>\n",
       "      <td>11.211523</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.712859</td>\n",
       "      <td>9.652075</td>\n",
       "      <td>28.317406</td>\n",
       "      <td>40.060784</td>\n",
       "      <td>108.168725</td>\n",
       "      <td>7.918501</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pelvic_incidence  pelvic_tilt  lumbar_lordosis_angle  sacral_slope  \\\n",
       "0         63.027817    22.552586              39.609117     40.475232   \n",
       "1         39.056951    10.060991              25.015378     28.995960   \n",
       "2         68.832021    22.218482              50.092194     46.613539   \n",
       "3         69.297008    24.652878              44.311238     44.644130   \n",
       "4         49.712859     9.652075              28.317406     40.060784   \n",
       "\n",
       "   pelvic_radius  degree_spondylolisthesis     class  \n",
       "0      98.672917                 -0.254400  Abnormal  \n",
       "1     114.405425                  4.564259  Abnormal  \n",
       "2     105.985135                 -3.530317  Abnormal  \n",
       "3     101.868495                 11.211523  Abnormal  \n",
       "4     108.168725                  7.918501  Abnormal  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vertebral.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pelvic_incidence</th>\n",
       "      <th>pelvic_tilt</th>\n",
       "      <th>lumbar_lordosis_angle</th>\n",
       "      <th>sacral_slope</th>\n",
       "      <th>pelvic_radius</th>\n",
       "      <th>degree_spondylolisthesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>60.496653</td>\n",
       "      <td>17.542822</td>\n",
       "      <td>51.930930</td>\n",
       "      <td>42.953831</td>\n",
       "      <td>117.920655</td>\n",
       "      <td>26.296694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.236520</td>\n",
       "      <td>10.008330</td>\n",
       "      <td>18.554064</td>\n",
       "      <td>13.423102</td>\n",
       "      <td>13.317377</td>\n",
       "      <td>37.559027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>26.147921</td>\n",
       "      <td>-6.554948</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>13.366931</td>\n",
       "      <td>70.082575</td>\n",
       "      <td>-11.058179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>46.430294</td>\n",
       "      <td>10.667069</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>33.347122</td>\n",
       "      <td>110.709196</td>\n",
       "      <td>1.603727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>58.691038</td>\n",
       "      <td>16.357689</td>\n",
       "      <td>49.562398</td>\n",
       "      <td>42.404912</td>\n",
       "      <td>118.268178</td>\n",
       "      <td>11.767934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>72.877696</td>\n",
       "      <td>22.120395</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>52.695888</td>\n",
       "      <td>125.467674</td>\n",
       "      <td>41.287352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>129.834041</td>\n",
       "      <td>49.431864</td>\n",
       "      <td>125.742385</td>\n",
       "      <td>121.429566</td>\n",
       "      <td>163.071041</td>\n",
       "      <td>418.543082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pelvic_incidence  pelvic_tilt  lumbar_lordosis_angle  sacral_slope  \\\n",
       "count        310.000000   310.000000             310.000000    310.000000   \n",
       "mean          60.496653    17.542822              51.930930     42.953831   \n",
       "std           17.236520    10.008330              18.554064     13.423102   \n",
       "min           26.147921    -6.554948              14.000000     13.366931   \n",
       "25%           46.430294    10.667069              37.000000     33.347122   \n",
       "50%           58.691038    16.357689              49.562398     42.404912   \n",
       "75%           72.877696    22.120395              63.000000     52.695888   \n",
       "max          129.834041    49.431864             125.742385    121.429566   \n",
       "\n",
       "       pelvic_radius  degree_spondylolisthesis  \n",
       "count     310.000000                310.000000  \n",
       "mean      117.920655                 26.296694  \n",
       "std        13.317377                 37.559027  \n",
       "min        70.082575                -11.058179  \n",
       "25%       110.709196                  1.603727  \n",
       "50%       118.268178                 11.767934  \n",
       "75%       125.467674                 41.287352  \n",
       "max       163.071041                418.543082  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vertebral.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Les librairies de Machine Learning telles que `sckitlearn` prennent en entrée des tableau numpy (pas des objets pandas). Créer un tableau numpy que vous nommerez `VertebralVar` pour les features et un vecteur numpy `VertebralClas` pour la variable de classe. Voir par exemple [ici](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html#pandas.DataFrame.to_numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pelvic_incidence  pelvic_tilt  lumbar_lordosis_angle  sacral_slope  \\\n",
      "0           63.027817    22.552586              39.609117     40.475232   \n",
      "1           39.056951    10.060991              25.015378     28.995960   \n",
      "2           68.832021    22.218482              50.092194     46.613539   \n",
      "3           69.297008    24.652878              44.311238     44.644130   \n",
      "4           49.712859     9.652075              28.317406     40.060784   \n",
      "..                ...          ...                    ...           ...   \n",
      "305         47.903565    13.616688              36.000000     34.286877   \n",
      "306         53.936748    20.721496              29.220534     33.215251   \n",
      "307         61.446597    22.694968              46.170347     38.751628   \n",
      "308         45.252792     8.693157              41.583126     36.559635   \n",
      "309         33.841641     5.073991              36.641233     28.767649   \n",
      "\n",
      "     pelvic_radius  degree_spondylolisthesis  \n",
      "0        98.672917                 -0.254400  \n",
      "1       114.405425                  4.564259  \n",
      "2       105.985135                 -3.530317  \n",
      "3       101.868495                 11.211523  \n",
      "4       108.168725                  7.918501  \n",
      "..             ...                       ...  \n",
      "305     117.449062                 -4.245395  \n",
      "306     114.365845                 -0.421010  \n",
      "307     125.670725                 -2.707880  \n",
      "308     118.545842                  0.214750  \n",
      "309     123.945244                 -0.199249  \n",
      "\n",
      "[310 rows x 6 columns]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      Abnormal\n",
       "1      Abnormal\n",
       "2      Abnormal\n",
       "3      Abnormal\n",
       "4      Abnormal\n",
       "         ...   \n",
       "305      Normal\n",
       "306      Normal\n",
       "307      Normal\n",
       "308      Normal\n",
       "309      Normal\n",
       "Name: class, Length: 310, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = list(Vertebral.columns)\n",
    "cols.pop()\n",
    "VertebralVar = Vertebral[cols]\n",
    "VertebralClas = Vertebral['class']\n",
    "print(VertebralVar)\n",
    "print('\\n\\n')\n",
    "VertebralClas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Découpage train / test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En apprentissage statistique, classiquement un prédicteur est ajusté sur une partie seulement des données et l'erreur de ce dernier est ensuite évaluée sur une autre partie des données disponibles. Ceci permet de ne pas utiliser les mêmes données pour ajuster et évaluer la qualité d'un prédicteur. Cette problématique est l'objet du prochain chapitre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En utilisant la fonction [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) de la librairie [`sklearn.model_selection`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection), sélectionner aléatoirement 60% des observations pour l'échantillon d'apprentissage et garder le reste pour l'échantillon de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310 186 124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "VertebralVar_train,VertebralVar_test,VertebralClas_train, VertebralClas_test = train_test_split(VertebralVar,VertebralClas,train_size=0.6)\n",
    "ntot = len(VertebralVar) ### longueur totale de l'échantillon -  TO DO ####\n",
    "ntrain = len(VertebralVar_train) ### longueur totale de l'échantillon d'apprentissage - TO DO ####\n",
    "ntest = len(VertebralVar_test) ### longueur totale de l'échantillon de test -TO DO ####\n",
    "print(ntot, ntrain, ntest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarque : on peut aussi le faire à la main avec la fonction [`sklearn.utils.shuffle`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des deux classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Extraire les deux sous-échantillons de classes respectives \"Abnormale\" et \"Normale\" pour les données d'apprentissage et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_abnormal = VertebralVar_train[VertebralClas_train == \"Abnormal\"]\n",
    "train_normal   = VertebralVar_train[VertebralClas_train == \"Normal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "n_AB = len(train_abnormal)\n",
    "n_NO = len(train_normal)\n",
    "print(n_AB)\n",
    "print(n_NO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Discriminante Linéaire (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons ajuster à la main le classifieur de l'analyse discriminante linéaire sur l'échantillon d'apprentissage et ensuite évaluer ses performances en considérant ses prédictions sur l'échantillon de test.\n",
    "\n",
    "- Pour calculer la matrice de covariance on peut utiliser \n",
    "par exemple la fonction [`empirical_covariance`](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.empirical_covariance.html#sklearn.covariance.empirical_covariance) de la librairie  [`sklearn.covariance`](http://scikit-learn.org/stable/modules/covariance.html).\n",
    "- Pour calculer la valeur de la densité d'une gaussienne multidimensionnelle en un point $x$ de $\\mathbb R ^d$ on peut utililser la fonction [`multivariate_normal`](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.multivariate_normal.html) de la librairie [`scipy.stats`](https://docs.scipy.org/doc/scipy/reference/stats.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'abord on calcule le centre et la matrice de covariance pour chacun des deux groupes  :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 309.70821206   91.56207438  219.6878246   218.14613768  -36.20270589\n",
      "   525.85945177]\n",
      " [  91.56207438  102.12271802   66.02361286  -10.56064363   26.43508417\n",
      "   121.84344301]\n",
      " [ 219.6878246    66.02361286  361.36428688  153.66421173   14.48896562\n",
      "   398.95628611]\n",
      " [ 218.14613768  -10.56064363  153.66421173  228.70678131  -62.63779005\n",
      "   404.01600877]\n",
      " [ -36.20270589   26.43508417   14.48896562  -62.63779005  208.63582552\n",
      "    73.92416486]\n",
      " [ 525.85945177  121.84344301  398.95628611  404.01600877   73.92416486\n",
      "  2142.70990972]] \n",
      "\n",
      " [[184.3710931   62.55143594 137.46485801 121.81965716 -64.37421199\n",
      "   15.45968662]\n",
      " [ 62.55143594  51.63430587  37.55259059  10.91713007 -26.17158551\n",
      "    7.16273523]\n",
      " [137.46485801  37.55259059 159.24374669  99.91226742 -26.87049165\n",
      "   13.00865171]\n",
      " [121.81965716  10.91713007  99.91226742 110.90252709 -38.20262648\n",
      "    8.29695139]\n",
      " [-64.37421199 -26.17158551 -26.87049165 -38.20262648  85.06361507\n",
      "   -1.33933807]\n",
      " [ 15.45968662   7.16273523  13.00865171   8.29695139  -1.33933807\n",
      "   33.60213597]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.covariance import empirical_covariance\n",
    "Cov_AB = empirical_covariance(train_abnormal)\n",
    "Cov_NO = empirical_covariance(train_normal)\n",
    "mean_AB = train_abnormal.mean()\n",
    "mean_NO = train_normal.mean()\n",
    "print(Cov_AB,'\\n\\n',Cov_NO,'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pelvic_incidence             63.728982\n",
      "pelvic_tilt                  19.271044\n",
      "lumbar_lordosis_angle        54.547379\n",
      "sacral_slope                 44.457938\n",
      "pelvic_radius               116.072614\n",
      "degree_spondylolisthesis     38.560392\n",
      "dtype: float64\n",
      "pelvic_incidence             52.742588\n",
      "pelvic_tilt                  13.434078\n",
      "lumbar_lordosis_angle        43.695151\n",
      "sacral_slope                 39.308509\n",
      "pelvic_radius               122.758333\n",
      "degree_spondylolisthesis      2.164741\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(mean_AB)\n",
    "print(mean_NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Puis on calcule la matrice de covariance intra (voir le cours ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[162.77906988  49.60303046 116.69427832 113.17603941 -26.90154562\n",
      "  221.66797231]\n",
      " [ 49.60303046  51.99027761  34.3791733   -2.38724714   6.18821124\n",
      "   52.01961309]\n",
      " [116.69427832  34.3791733  179.6544728   82.31510502   1.08857594\n",
      "  168.40920663]\n",
      " [113.17603941  -2.38724714  82.31510502 115.56328656 -33.08975686\n",
      "  169.64835923]\n",
      " [-26.90154562   6.18821124   1.08857594 -33.08975686 102.46015339\n",
      "   30.5157258 ]\n",
      " [221.66797231  52.01961309 168.40920663 169.64835923  30.5157258\n",
      "  897.8222584 ]]\n"
     ]
    }
   ],
   "source": [
    "Intra_Cov = (n_AB/ntot)*Cov_AB + (n_NO/ntot)*Cov_NO\n",
    "print(Intra_Cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour une observation $x \\in \\mathbb R^6$ (décrite par ses 6 features), la régle du MAP (Maximum A Posteriori) dans le cas de l'analyse discriminante linéaire consiste à choisir la catégorie $\\hat y (x) = \\hat k $ qui maximise (en $k$) \n",
    "$$ score_k(x) = \\hat \\pi_k \\hat f_k(x) $$\n",
    "où :\n",
    "- $k$ est le numéro de la classe ;\n",
    "- $\\hat \\pi_k$ est la proportion observée de la classe $k$, \n",
    "- $\\hat f_k$ est la densité gaussienne multidimensionnelle de la classe $k$ (avec pour paramètre de centrage $\\mu_k$ et pour matrice de covariance Intra (pour toutes les classes).\n",
    "\n",
    "On calcule tout d'abord pour toutes les données de test les valeurs des scores sur les deux catégories :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[np.float64(76.26468012164483), np.float64(204.80135574252068)],\n",
       " [np.float64(43.914192623063755), np.float64(153.99042108715236)],\n",
       " [np.float64(67.74030713070319), np.float64(219.74544781506717)],\n",
       " [np.float64(57.136191122551864), np.float64(178.5117268121308)],\n",
       " [np.float64(82.10517491595024), np.float64(200.18522679866015)],\n",
       " [np.float64(55.4733070567287), np.float64(198.79895071745574)],\n",
       " [np.float64(64.65295152141847), np.float64(211.11544893742263)],\n",
       " [np.float64(68.90040303891661), np.float64(273.86989173353373)],\n",
       " [np.float64(55.710484230743205), np.float64(155.06676240907706)],\n",
       " [np.float64(60.96914465343947), np.float64(205.84406059295594)],\n",
       " [np.float64(68.50716354368856), np.float64(161.70357830882728)],\n",
       " [np.float64(53.09509055581806), np.float64(162.23845902145564)],\n",
       " [np.float64(59.08643379059289), np.float64(164.20208074957787)],\n",
       " [np.float64(58.36854807901331), np.float64(159.68692969218552)],\n",
       " [np.float64(60.836380912600035), np.float64(182.66123758316695)],\n",
       " [np.float64(62.06631842555147), np.float64(179.33631266713797)],\n",
       " [np.float64(62.44667865367741), np.float64(189.5766240545841)],\n",
       " [np.float64(74.07781049594419), np.float64(218.8631196251577)],\n",
       " [np.float64(52.503901753892606), np.float64(171.56710489527046)],\n",
       " [np.float64(69.14014434334027), np.float64(232.30776994735658)],\n",
       " [np.float64(61.30077896124352), np.float64(162.83113433480918)],\n",
       " [np.float64(69.53159374274915), np.float64(171.62361948997437)],\n",
       " [np.float64(57.93828073897534), np.float64(181.27018475592791)],\n",
       " [np.float64(63.69267318598362), np.float64(264.06352362096965)],\n",
       " [np.float64(59.516624799781894), np.float64(195.9581856733626)],\n",
       " [np.float64(64.86238800952346), np.float64(196.4533628827399)],\n",
       " [np.float64(45.33050494672865), np.float64(98.72206788756608)],\n",
       " [np.float64(61.123647912315974), np.float64(211.3870296707815)],\n",
       " [np.float64(75.80601306631948), np.float64(226.44548879996597)],\n",
       " [np.float64(70.31658553468665), np.float64(192.94770842850863)],\n",
       " [np.float64(79.54565608172348), np.float64(272.9145738637513)],\n",
       " [np.float64(67.26504554561474), np.float64(232.7904230511254)],\n",
       " [np.float64(73.02098889517734), np.float64(217.3658636590904)],\n",
       " [np.float64(63.011153595025135), np.float64(208.65815717042148)],\n",
       " [np.float64(59.94928934515123), np.float64(189.89944563866317)],\n",
       " [np.float64(59.175457452509036), np.float64(228.46482756049096)],\n",
       " [np.float64(42.41632179598459), np.float64(67.66381814763605)],\n",
       " [np.float64(55.11915505197034), np.float64(172.44075805754125)],\n",
       " [np.float64(64.87795567084471), np.float64(201.03681058496892)],\n",
       " [np.float64(54.2673576844078), np.float64(168.6907912126249)],\n",
       " [np.float64(71.51686590219984), np.float64(157.48193050981223)],\n",
       " [np.float64(65.75200806310535), np.float64(233.20835960774872)],\n",
       " [np.float64(72.91302347634124), np.float64(231.76229410678803)],\n",
       " [np.float64(50.60571604176734), np.float64(190.93327592559814)],\n",
       " [np.float64(41.29609059853614), np.float64(115.14498592600881)],\n",
       " [np.float64(28.47810123880994), np.float64(46.90859583825528)],\n",
       " [np.float64(44.301725908791596), np.float64(142.65093306512296)],\n",
       " [np.float64(73.61730545593096), np.float64(165.2857321953839)],\n",
       " [np.float64(59.8990112295411), np.float64(186.42606007554292)],\n",
       " [np.float64(78.96289640661931), np.float64(150.49029160351216)],\n",
       " [np.float64(66.02258816393227), np.float64(202.19819480241)],\n",
       " [np.float64(66.2857259586141), np.float64(162.36921428949296)],\n",
       " [np.float64(48.47334044988169), np.float64(177.87536800340473)],\n",
       " [np.float64(63.09062091269665), np.float64(157.82538621932923)],\n",
       " [np.float64(55.44777632699483), np.float64(191.1021514988011)],\n",
       " [np.float64(55.14594174161688), np.float64(158.22630445440024)],\n",
       " [np.float64(72.03586426036568), np.float64(221.63106928080975)],\n",
       " [np.float64(63.12723231885833), np.float64(175.4381881436771)],\n",
       " [np.float64(53.635945278305044), np.float64(219.3556910759276)],\n",
       " [np.float64(50.663900450044494), np.float64(186.24604231179416)],\n",
       " [np.float64(64.9440495059513), np.float64(95.51072030574738)],\n",
       " [np.float64(49.7423478007819), np.float64(61.10296781048834)],\n",
       " [np.float64(61.05414249733225), np.float64(234.3180422341889)],\n",
       " [np.float64(65.39114518466307), np.float64(194.47007265747249)],\n",
       " [np.float64(66.84192935909745), np.float64(177.3905482447213)],\n",
       " [np.float64(28.089220444204003), np.float64(40.15630030692276)],\n",
       " [np.float64(73.75116369084358), np.float64(136.8991577892071)],\n",
       " [np.float64(50.452213602109296), np.float64(178.41140889436662)],\n",
       " [np.float64(55.68634380362131), np.float64(156.18288388908564)],\n",
       " [np.float64(53.894144786956204), np.float64(192.61534175337016)],\n",
       " [np.float64(58.69741026113992), np.float64(180.06753758773863)],\n",
       " [np.float64(46.620494152970096), np.float64(156.85783517420947)],\n",
       " [np.float64(67.96471309943958), np.float64(153.33894392908275)],\n",
       " [np.float64(78.82907361056792), np.float64(267.2626430100268)],\n",
       " [np.float64(58.298189322549334), np.float64(241.19871657401978)],\n",
       " [np.float64(70.5647815748504), np.float64(182.67731131673514)],\n",
       " [np.float64(56.913176700092336), np.float64(142.54894255906999)],\n",
       " [np.float64(47.881505172319194), np.float64(123.41061773956477)],\n",
       " [np.float64(73.77529533605926), np.float64(193.0891074916786)],\n",
       " [np.float64(50.82274099773862), np.float64(113.39564433381616)],\n",
       " [np.float64(60.908948728201395), np.float64(209.7729193872279)],\n",
       " [np.float64(67.26189279995528), np.float64(156.6754465258187)],\n",
       " [np.float64(50.6957292022977), np.float64(96.4161492312765)],\n",
       " [np.float64(57.80902631892026), np.float64(175.74506412879288)],\n",
       " [np.float64(64.01124576569204), np.float64(246.40250588268697)],\n",
       " [np.float64(61.15966909643397), np.float64(205.92134649366795)],\n",
       " [np.float64(67.15877767544374), np.float64(135.1551387316054)],\n",
       " [np.float64(26.032880118900394), np.float64(10.502004115513529)],\n",
       " [np.float64(73.7845262091993), np.float64(229.4321899271077)],\n",
       " [np.float64(56.39257998656616), np.float64(122.58745951638281)],\n",
       " [np.float64(43.09197929560982), np.float64(89.99727600128114)],\n",
       " [np.float64(83.19097348089932), np.float64(251.46127153993308)],\n",
       " [np.float64(54.704425101370035), np.float64(128.73728498131692)],\n",
       " [np.float64(66.86488032524143), np.float64(189.14250523284733)],\n",
       " [np.float64(60.02844931170479), np.float64(172.62413390041053)],\n",
       " [np.float64(75.06101229597485), np.float64(208.28207787186562)],\n",
       " [np.float64(75.10657351402702), np.float64(221.25238629490792)],\n",
       " [np.float64(54.73476708741295), np.float64(275.56481426642)],\n",
       " [np.float64(47.10115415588156), np.float64(144.64921519965588)],\n",
       " [np.float64(61.62320208988753), np.float64(182.83035085410535)],\n",
       " [np.float64(67.98230766583369), np.float64(156.8877025736934)],\n",
       " [np.float64(63.35289530115337), np.float64(214.60518530350834)],\n",
       " [np.float64(62.3281528044878), np.float64(195.93209313288986)],\n",
       " [np.float64(56.35731221068785), np.float64(151.3625664568013)],\n",
       " [np.float64(64.7973464322546), np.float64(285.37492048800647)],\n",
       " [np.float64(43.622212458487084), np.float64(88.05772634655892)],\n",
       " [np.float64(54.56345749730612), np.float64(169.13492734790503)],\n",
       " [np.float64(67.45946460481868), np.float64(160.18851310074984)],\n",
       " [np.float64(55.387627975634715), np.float64(155.30192855805814)],\n",
       " [np.float64(66.73287937263161), np.float64(204.98321248651206)],\n",
       " [np.float64(67.20332396970265), np.float64(205.89932047635972)],\n",
       " [np.float64(66.74712564967707), np.float64(233.66778647401094)],\n",
       " [np.float64(62.85069172139414), np.float64(152.18866631821453)],\n",
       " [np.float64(53.09542844778993), np.float64(191.8854807081139)],\n",
       " [np.float64(74.14514453912639), np.float64(244.00034147561252)],\n",
       " [np.float64(57.46696033174307), np.float64(151.6627445590189)],\n",
       " [np.float64(68.1126833366849), np.float64(235.6328131562537)],\n",
       " [np.float64(51.77426197464014), np.float64(150.89721608967244)],\n",
       " [np.float64(64.92696541585396), np.float64(162.93459996641337)],\n",
       " [np.float64(65.58671741932416), np.float64(164.15776740074813)],\n",
       " [np.float64(65.21462107085758), np.float64(237.7112938395208)],\n",
       " [np.float64(65.61447904081218), np.float64(169.06739359409272)],\n",
       " [np.float64(67.28923206971162), np.float64(262.7679717472261)],\n",
       " [np.float64(43.37700782841206), np.float64(89.05558036607414)]]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_LDA_test = [   [np.transpose(x).dot(np.linalg.inv(Cov_AB)).dot(mean_AB) + np.log(n_AB/ntot)-0.5*np.transpose(mean_AB).dot(np.linalg.inv(Cov_AB)).dot(mean_AB) , np.transpose(x).dot(np.linalg.inv(Cov_NO)).dot(mean_NO) + np.log(n_NO/ntot)-0.5*np.transpose(mean_NO).dot(np.linalg.inv(Cov_NO)).dot(mean_NO)    ]  for x in VertebralVar_test.values]\n",
    "score_LDA_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et on choisit la classe qui maximise le score (pour chaque élement des données de test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False,  True, False,  True, False,  True,\n",
       "       False,  True, False,  True,  True,  True, False, False,  True,\n",
       "       False,  True, False,  True,  True, False,  True, False, False,\n",
       "       False, False,  True,  True, False,  True, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False,  True, False,  True, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "        True,  True, False, False, False, False, False,  True, False,\n",
       "        True, False, False,  True, False, False, False, False,  True,\n",
       "       False, False, False,  True, False,  True,  True, False,  True,\n",
       "       False,  True, False, False, False, False,  True, False, False,\n",
       "        True, False, False,  True,  True, False, False, False,  True,\n",
       "        True,  True,  True,  True,  True, False, False, False, False,\n",
       "       False,  True,  True, False,  True, False, False])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_LDA_test = []\n",
    "for k in range (len(score_LDA_test)):\n",
    "    if score_LDA_test[k][0]>score_LDA_test[k][1] :\n",
    "        pred_LDA_test.append(['Abnormal'])\n",
    "    else :\n",
    "        pred_LDA_test.append(['Normal'])\n",
    "pred_LDA_test\n",
    "VertebralClas_test.values == np.array(pred_LDA_test).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice de confusion est une matrice qui synthétise les performances d'une régle de classification. Chaque ligne correspond à une classe réelle, chaque colonne correspond à une classe estimée. La cellule (ligne L, colonne C) contient le nombre d'éléments de la classe réelle L qui ont été estimés comme appartenant à la classe C. Voir par exemple [ici](https://fr.wikipedia.org/wiki/Matrice_de_confusion).\n",
    "\n",
    "\n",
    "> Evaluer les performances de la méthode sur l'échantillon test. Vous pourrez utiliser la fonction [`confusion_matrix`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) de la librairie [`sklearn.metrics`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        ],\n",
       "       [0.98765432, 0.01234568]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "vraie_classe_test = VertebralClas_test \n",
    "cnf_matrix_LDA_test = confusion_matrix(np.array(vraie_classe_test).ravel(), np.array(pred_LDA_test).ravel(), labels=['Normal', 'Abnormal'])\n",
    "cnf_matrix_LDA_test.astype('float') / cnf_matrix_LDA_test.sum(axis=1).reshape(-1,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vérifier que l'évaluation de la méthode sur les données d'apprentissage donne un résulat sensiblement plus optimiste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        ],\n",
       "       [0.99224806, 0.00775194]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_LDA_train =  [[np.transpose(x).dot(np.linalg.inv(Cov_AB)).dot(mean_AB) + np.log(n_AB/ntot)-0.5*np.transpose(mean_AB).dot(np.linalg.inv(Cov_AB)).dot(mean_AB) , np.transpose(x).dot(np.linalg.inv(Cov_NO)).dot(mean_NO) + np.log(n_NO/ntot)-0.5*np.transpose(mean_NO).dot(np.linalg.inv(Cov_NO)).dot(mean_NO)    ]  for x in VertebralVar_train.values]\n",
    "pred_LDA_train = []\n",
    "for k in range (len(score_LDA_train)):\n",
    "    if score_LDA_train[k][0]>score_LDA_train[k][1] :\n",
    "        pred_LDA_train.append(['Abnormal'])\n",
    "    else :\n",
    "        pred_LDA_train.append(['Normal'])\n",
    "vraie_classe_train = np.array(VertebralClas_train).ravel()\n",
    "cnf_matrix_LDA_train =  confusion_matrix(vraie_classe_train,pred_LDA_train,labels=['Normal','Abnormal'])\n",
    "cnf_matrix_LDA_train.astype('float') / cnf_matrix_LDA_train.sum(axis=1).reshape(-1,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe bien sûr une fonction scikit-learn pour la méthode LDA : voir [ici](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ajuster un modèle de régression logistique sur les données d'apprentissage en utilisant la fonction  `sklearn.linear_model.LogisticRegression`, voir [ici](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).   \n",
    "> Par défaut une penalty $l_2$ est présente dans la fonction de perte (voir la doc). Pour obtenir un ajustement pour la regression logistique standard, il faut donc  prendre garder à préciser `penalty=‘none’` dans les options de la fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train = VertebralVar_train.values\n",
    "y_train = np.ravel(VertebralClas_train)\n",
    "\n",
    "logreg = LogisticRegression(max_iter = 1000, penalty = None).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Afficher les coefficients estimés de la regression logistique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Afficher avec la fonction `predict_proba` les estimations des probabilités a posteriori sur l'échantillon de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Afficher la matrice de confusion et comparer avec les résultats avec ceux de la LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8372093 , 0.1627907 ],\n",
       "       [0.09876543, 0.90123457]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = VertebralVar_test.values\n",
    "y_test = np.ravel(VertebralClas_test)\n",
    "pred_log_test = logreg.predict(X_test)\n",
    "cnf_matrix_LDA_train =  confusion_matrix(y_test,pred_log_test,labels=['Normal','Abnormal'])\n",
    "cnf_matrix_LDA_train.astype('float') / cnf_matrix_LDA_train.sum(axis=1).reshape(-1,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71929825, 0.28070175],\n",
       "       [0.10852713, 0.89147287]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = VertebralVar_train.values\n",
    "y_test = np.ravel(VertebralClas_train)\n",
    "pred_log_test = logreg.predict(X_test)\n",
    "cnf_matrix_LDA_train =  confusion_matrix(y_test,pred_log_test,labels=['Normal','Abnormal'])\n",
    "cnf_matrix_LDA_train.astype('float') / cnf_matrix_LDA_train.sum(axis=1).reshape(-1,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bonus** (à faire à la fin si vous le souhaitez) : recoder par vous-même un prédicteur de régression logistique et comparer vos prévisions avec celles obtenues par la fonction de sklearn. Notez que sur ce jeu de données l'estimation du modèle de régression logistique est relativement instable du fait que la fonction de vraisembance correspondante est assez ``plate\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Discriminante Quadratique (QDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reprendre et adapter les codes précédents (de la LDA) pour ajuster cette fois un classifieur par analyse discriminante quadratique sur les données d'apprentissage. \n",
    "> \n",
    "> Evaluer la qualité du classifieur sur les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124 124\n",
      "Matrice de confusion QDA :\n",
      " [[38  5]\n",
      " [17 64]]\n",
      "\n",
      "Matrice normalisée :\n",
      " [[0.88372093 0.11627907]\n",
      " [0.20987654 0.79012346]]\n"
     ]
    }
   ],
   "source": [
    "score_QDA_test = [\n",
    "    [ np.log(n_AB/ntot) - 0.5 * np.transpose(x - mean_AB).dot(np.linalg.inv(Cov_AB)).dot(x - mean_AB) - 0.5 * np.log(abs(np.linalg.det(Cov_AB))),\n",
    "         np.log(n_NO/ntot) - 0.5 * np.transpose(x - mean_NO).dot(np.linalg.inv(Cov_NO)).dot(x - mean_NO) - 0.5 * np.log(abs(np.linalg.det(Cov_NO)))\n",
    "    ] for x in VertebralVar_test.values   ] # <-- CORRECTION ICI\n",
    "\n",
    "# Prédictions\n",
    "pred_QDA_test = []\n",
    "for k in range(len(score_QDA_test)):\n",
    "    if score_QDA_test[k][0] > score_QDA_test[k][1]:\n",
    "        pred_QDA_test.append('Abnormal')   # <-- PAS de crochets\n",
    "    else:\n",
    "        pred_QDA_test.append('Normal')\n",
    "\n",
    "# Vérification des dimensions\n",
    "print(len(VertebralClas_test), len(pred_QDA_test))  # doivent être égales\n",
    "\n",
    "# Matrice de confusion\n",
    "cnf_matrix_QDA_test = confusion_matrix( np.array(VertebralClas_test).ravel(), np.array(pred_QDA_test).ravel(),  labels=['Normal', 'Abnormal'])\n",
    "\n",
    "# Normalisation ligne par ligne\n",
    "cnf_matrix_QDA_test_norm = (cnf_matrix_QDA_test.astype('float')/ cnf_matrix_QDA_test.sum(axis=1).reshape(-1, 1))\n",
    "\n",
    "print(\"Matrice de confusion QDA :\\n\", cnf_matrix_QDA_test)\n",
    "print(\"\\nMatrice normalisée :\\n\", cnf_matrix_QDA_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion LDA :\n",
      " [[1.         0.        ]\n",
      " [0.98765432 0.01234568]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(VertebralVar_train, VertebralClas_train)\n",
    "\n",
    "pred_LDA = lda.predict(VertebralVar_test)\n",
    "cnf_matrix_LDA = confusion_matrix(VertebralClas_test, pred_LDA, labels=['Normal', 'Abnormal'])\n",
    "cnf_matrix_LDA_test_norm = (cnf_matrix_LDA_test.astype('float')/ cnf_matrix_LDA_test.sum(axis=1).reshape(-1, 1))\n",
    "\n",
    "print(\"Matrice de confusion LDA :\\n\", cnf_matrix_LDA_test_norm)\n",
    "\n",
    "#print(\"\\nRapport de classification LDA :\\n\")\n",
    "#print(classification_report(VertebralClas_test, pred_LDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion QDA :\n",
      " [[0.88372093 0.11627907]\n",
      " [0.20987654 0.79012346]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kassi\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kassi\\anaconda3\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "qda = QuadraticDiscriminantAnalysis()\n",
    "qda.fit(VertebralVar_train, VertebralClas_train)\n",
    "\n",
    "pred_QDA = qda.predict(VertebralVar_test)\n",
    "cnf_matrix_QDA = confusion_matrix(VertebralClas_test, pred_QDA, labels=['Normal', 'Abnormal'])\n",
    "cnf_matrix_QDA_test_norm = (cnf_matrix_QDA_test.astype('float')/ cnf_matrix_QDA_test.sum(axis=1).reshape(-1, 1))\n",
    "\n",
    "print(\"Matrice de confusion QDA :\\n\", cnf_matrix_QDA_test_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Abnormal'], dtype=object)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = VertebralVar_train.iloc[[2]]\n",
    "qda.predict(x)\n",
    "#VertebralClas_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe bien sûr une fonction scikit-learn pour la méthode QDA : voir [ici](\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant ajuster un classifieur naif bayesien sur les données d'apprentissage.\n",
    "\n",
    "Pour une observation $x \\in \\mathbb R^6$, la régle du MAP consiste cette fois à choisir la catégorie $\\hat y (x) = \\hat k $ qui maximise (en $k$) \n",
    "$$ score_k(x) = \\hat \\pi_k \\prod_{j=1} ^6  \\hat f_{k,j}(x_j)   $$\n",
    "où :\n",
    "- $k$ est le numéro de la classe ;\n",
    "- $\\hat \\pi_k$ est la proportion observée de la classe $k$, \n",
    "- $\\hat f_{k,j} $ est la densité gaussienne univariée de la classe $k$ pour la variable $j$. Les paramètres de cette loi valent (ajustés par maximum de vraisemblance) :\n",
    "    - $\\hat \\mu_{k,j}$ : la moyenne empirique de la variable $X^j$ restreinte à la classe k,\n",
    "    - $ \\hat \\sigma^2_{k,j}$ : la variance empirique de la variable $X^j$ restreinte à la classe k.\n",
    "    \n",
    "Noter que la fonction $x \\mapsto  \\prod_{j=1} ^6  f_{k,j}(x_j) $ peut aussi être vue comme une densité gaussienne multidimensionnelle de moyenne $(\\mu_{k,1}, \\dots, \\mu_{k,6})$ et de matrice de covariance diagonale $diag(\\hat \\sigma^2_{k,1},\\dots,\\hat  \\sigma^2_{k,6})$. Cette remarque évite de devoir calculer le produit de 6 densités univariées, à la place on calcule plus directement la valeur de la densité multidimensionnelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul des moyennes et des variances de chaque variable pour chacun des deux groupes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63.72898208193798, 19.271044487263566, 54.54737868790698, 44.45793759449613, 116.0726138591473, 38.56039164388372] \n",
      "\n",
      " pelvic_incidence             52.742588\n",
      "pelvic_tilt                  13.434078\n",
      "lumbar_lordosis_angle        43.695151\n",
      "sacral_slope                 39.308509\n",
      "pelvic_radius               122.758333\n",
      "degree_spondylolisthesis      2.164741\n",
      "dtype: float64 \n",
      "\n",
      " pelvic_incidence             312.127807\n",
      "pelvic_tilt                  102.920552\n",
      "lumbar_lordosis_angle        364.187445\n",
      "sacral_slope                 230.493553\n",
      "pelvic_radius                210.265793\n",
      "degree_spondylolisthesis    2159.449831\n",
      "dtype: float64 \n",
      "\n",
      " pelvic_incidence            187.663434\n",
      "pelvic_tilt                  52.556347\n",
      "lumbar_lordosis_angle       162.087385\n",
      "sacral_slope                112.882929\n",
      "pelvic_radius                86.582608\n",
      "degree_spondylolisthesis     34.202174\n",
      "dtype: float64 \n",
      "\n",
      " [[ 309.70821206   91.56207438  219.6878246   218.14613768  -36.20270589\n",
      "   525.85945177]\n",
      " [  91.56207438  102.12271802   66.02361286  -10.56064363   26.43508417\n",
      "   121.84344301]\n",
      " [ 219.6878246    66.02361286  361.36428688  153.66421173   14.48896562\n",
      "   398.95628611]\n",
      " [ 218.14613768  -10.56064363  153.66421173  228.70678131  -62.63779005\n",
      "   404.01600877]\n",
      " [ -36.20270589   26.43508417   14.48896562  -62.63779005  208.63582552\n",
      "    73.92416486]\n",
      " [ 525.85945177  121.84344301  398.95628611  404.01600877   73.92416486\n",
      "  2142.70990972]] \n",
      "\n",
      " [[184.3710931   62.55143594 137.46485801 121.81965716 -64.37421199\n",
      "   15.45968662]\n",
      " [ 62.55143594  51.63430587  37.55259059  10.91713007 -26.17158551\n",
      "    7.16273523]\n",
      " [137.46485801  37.55259059 159.24374669  99.91226742 -26.87049165\n",
      "   13.00865171]\n",
      " [121.81965716  10.91713007  99.91226742 110.90252709 -38.20262648\n",
      "    8.29695139]\n",
      " [-64.37421199 -26.17158551 -26.87049165 -38.20262648  85.06361507\n",
      "   -1.33933807]\n",
      " [ 15.45968662   7.16273523  13.00865171   8.29695139  -1.33933807\n",
      "   33.60213597]]\n"
     ]
    }
   ],
   "source": [
    "mean_AB = train_abnormal.mean(axis=0)\n",
    "mean_NO = train_normal.mean(axis=0)\n",
    "\n",
    "# variances estimées coord par coord pour AB (sur le train) :\n",
    "var_AB = train_abnormal.var(axis=0)\n",
    "# variances estimées coord par coord pour NO (sur le train) :\n",
    "var_NO = train_normal.var(axis=0)\n",
    "\n",
    "# on forme les matrices de covariance (matrices diagonales car indep) :\n",
    "Cov_NB_AB =  empirical_covariance(train_abnormal) \n",
    "Cov_NB_NO =  empirical_covariance(train_normal)\n",
    "\n",
    "print(list(mean_AB),'\\n\\n',mean_NO,'\\n\\n',var_AB,'\\n\\n',var_NO,'\\n\\n',Cov_NB_AB,'\\n\\n',Cov_NB_NO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul du \"score\" sur chaque groupe pour chaque element des données test : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Normal',\n",
       " 'Abnormal',\n",
       " 'Abnormal']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrices de covariance diagonales (les variances seules suffisent pour Naïve Bayes)\n",
    "Cov_NB_AB = np.diag(var_AB)\n",
    "Cov_NB_NO = np.diag(var_NO)\n",
    "\n",
    "# Scores Naïve Bayes sur le train\n",
    "score_NB_train = [\n",
    "    [\n",
    "        (n_AB / ntot) * np.prod(1 / np.sqrt(2*np.pi*var_AB) * np.exp(-0.5 * ((np.array(x) - mean_AB)/np.sqrt(var_AB))**2)),\n",
    "        (n_NO / ntot) * np.prod(1 / np.sqrt(2*np.pi*var_NO) * np.exp(-0.5 * ((np.array(x) - mean_NO)/np.sqrt(var_NO))**2))\n",
    "    ]\n",
    "    for x in VertebralVar_test.values\n",
    "]\n",
    "\n",
    "# Prédiction\n",
    "pred_NB_train = ['Abnormal' if s[0] > s[1] else 'Normal' for s in score_NB_train]\n",
    "pred_NB_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array(VertebralClas_test).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array(pred_NB_train).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage de la matrice de confusion sur les données de test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90697674, 0.09302326],\n",
       "       [0.2345679 , 0.7654321 ]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnf_matrix_NB_test = confusion_matrix(np.array(VertebralClas_test).ravel(), np.array(pred_NB_train).ravel(),labels=['Normal','Abnormal'])\n",
    "cnf_matrix_NB_test.astype('float') / cnf_matrix_NB_test.sum(axis=1).reshape(-1,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  Il existe bien sûr une fonction scikit-learn  pour la méthode Naive Bayes : voir [ici](http://scikit-learn.org/stable/modules/naive_bayes.html). Vérifier que votre prédicteur donne la même réponse de cette fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90697674, 0.09302326],\n",
       "       [0.2345679 , 0.7654321 ]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(VertebralVar_train,VertebralClas_train)\n",
    "y = gnb.predict(VertebralVar_test)\n",
    "cnf_matrix_NB_test = confusion_matrix(VertebralClas_test,y,labels=['Normal','Abnormal'])\n",
    "cnf_matrix_NB_test.astype('float') / cnf_matrix_NB_test.sum(axis=1).reshape(-1,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifieur par plus proches voisins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est préférable d'utiliser la structure de données de type [k-d tree](https://en.wikipedia.org/wiki/K-d_tree) pour effectuer des requêtes de plus proches voisins dans un nuage de points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Contruction du k-d tree pour les données train (pour la métrique euclidienne) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "tree =  ### TO DO ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Rechercher les 10 plus proches voisins dans les données d'apprentissage du premier point des données de test et afficher les classes de ces observations voisines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_voisins =  tree.query(### TO DO ####)\n",
    "print(indices_voisins)\n",
    "classes_voisins = ### TO DO ####\n",
    "print(classes_voisins)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le classifieur par plus proches vosins, la prediction est la classe majoritaire des k plus proches voisins.\n",
    "\n",
    "> Donner la prédiction pour le premier point de test par vote majoritaire sur ses 10 plus proches voisins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Donner la prediction du classifieur ppv pour toutes les données de test. Evaluer la qualité du classifieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_class = ### CHOISIR  ####  #nombre de plus proche voisins utilisés\n",
    "pred_kNN_test =  ### TO DO ####\n",
    "cnf_matrix_kNN =### TO DO ####\n",
    "cnf_matrix_kNN.astype('float') / cnf_matrix_kNN.sum(axis=1).reshape(-1,1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe bien sûr une fonction scikit-learn pour le classifieur plus proche voisin, voir [ici](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
